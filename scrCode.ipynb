{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "     ---------------------------------------- 0.0/250.0 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/250.0 kB ? eta -:--:--\n",
      "     ---- -------------------------------- 30.7/250.0 kB 435.7 kB/s eta 0:00:01\n",
      "     ---- -------------------------------- 30.7/250.0 kB 435.7 kB/s eta 0:00:01\n",
      "     ---- -------------------------------- 30.7/250.0 kB 435.7 kB/s eta 0:00:01\n",
      "     ------ ------------------------------ 41.0/250.0 kB 163.4 kB/s eta 0:00:02\n",
      "     ------------ ------------------------ 81.9/250.0 kB 305.0 kB/s eta 0:00:01\n",
      "     ---------------- ------------------- 112.6/250.0 kB 385.0 kB/s eta 0:00:01\n",
      "     ---------------- ------------------- 112.6/250.0 kB 385.0 kB/s eta 0:00:01\n",
      "     -------------------- --------------- 143.4/250.0 kB 355.0 kB/s eta 0:00:01\n",
      "     ---------------------------- ------- 194.6/250.0 kB 436.8 kB/s eta 0:00:01\n",
      "     ------------------------------------ 250.0/250.0 kB 479.5 kB/s eta 0:00:00\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import csv\n",
    "\n",
    "def products_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # print(soup.prettify())\n",
    "\n",
    "    # Targeting the <ul> element with id \"product-grid\"\n",
    "    product_grid = soup.find('ul', id='product-grid')\n",
    "\n",
    "    # Targeting the <li> elements within the <ul> element\n",
    "    li_elements = product_grid.find_all('li')\n",
    "\n",
    "    # Extracting the links from the <li> elements and joining with the base URL\n",
    "    base_url = 'https://watchcollectors.co.uk'  # Replace with the actual base URL\n",
    "    all_product = [urljoin(base_url, li.find('a')['href']) for li in li_elements if li.find('a')]\n",
    "\n",
    "    return all_product\n",
    "\n",
    "def products_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Targeting the <ul> element with id \"product-grid\"\n",
    "    product_grid = soup.find('ul', id='product-grid')\n",
    "\n",
    "    # Targeting the <li> elements within the <ul> element\n",
    "    li_elements = product_grid.find_all('li')\n",
    "\n",
    "    # Extracting the links from the <li> elements and joining with the base URL\n",
    "    base_url = 'https://watchcollectors.co.uk'  # Replace with the actual base URL\n",
    "    all_product = [urljoin(base_url, li.find('a')['href']) for li in li_elements if li.find('a')]\n",
    "\n",
    "    return all_product\n",
    "\n",
    "# def extract_all_product_links(base_url):\n",
    "#     all_product_links = []\n",
    "\n",
    "#     # Loop through pages (assuming there are 10 pages, you can modify the range accordingly)\n",
    "#     for page_number in range(1, 35):\n",
    "#         # Construct the URL for the current page\n",
    "#         page_url = f'{base_url}?page={page_number}'\n",
    "        \n",
    "#         # Call the products_links function for the current page\n",
    "#         product_links_on_page = products_links(page_url)\n",
    "        \n",
    "#         # Append the product links to the overall list\n",
    "#         all_product_links.extend(product_links_on_page)\n",
    "\n",
    "#     return all_product_links\n",
    "\n",
    "# # Replace 'https://watchcollectors.co.uk/collections/all' with the actual base URL\n",
    "# base_url = 'https://watchcollectors.co.uk/collections/all'\n",
    "# all_product_links = extract_all_product_links(base_url)\n",
    "\n",
    "# # Print the result\n",
    "# print(all_product_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_product_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress FutureWarnings for :contains\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"soupsieve\")\n",
    "\n",
    "imgUrls=[];productName=[];Price=[];modelNumber=[];modelYear=[];Gender=[];Diameter=[]\n",
    "originalBox=[];originalPaper=[];productUrls=[]\n",
    "\n",
    "def Product_details(links):\n",
    "    c = 0\n",
    "    for i in links:\n",
    "\n",
    "        response = requests.get(i)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        try:\n",
    "            # image_urls = [img['data-src'] for img in soup.select('.media img[data-src]')]\n",
    "            # imgUrls.append(image_urls)\n",
    "            image_urls = [urljoin('https://', img['data-src']) for img in soup.select('.media img[data-src]')]\n",
    "            imgUrls.append(image_urls)\n",
    "        except:\n",
    "            imgUrls.append('')\n",
    "\n",
    "        productUrls.append(i)\n",
    "        \n",
    "        try:\n",
    "            # product_title = soup.select_one('.product__title').get_text(strip=True)\n",
    "            # productName.append(product_title)\n",
    "            product_title = soup.find('td', string='Model').find_next('td').get_text(strip=True)\n",
    "            productName.append(product_title)\n",
    "\n",
    "        except:\n",
    "            productName.append('')\n",
    "            \n",
    "        try:\n",
    "            price = soup.find('span', {'class': 'price-item--regular'}).text.strip()\n",
    "            print(price)\n",
    "            Price.append(price)\n",
    "        except:\n",
    "            Price.append('')\n",
    "\n",
    "        \n",
    "        try:\n",
    "            model_number = soup.find('td', string='Model Number').find_next('td').get_text(strip=True)\n",
    "            modelNumber.append(model_number)\n",
    "        except:\n",
    "            modelNumber.append('')\n",
    "    \n",
    "        try:\n",
    "            model_year = soup.find('td', string='Year').find_next('td').get_text(strip=True)\n",
    "            modelYear.append(model_year )\n",
    "        except:\n",
    "            modelYear.append('')\n",
    "    \n",
    "        try:\n",
    "            gender = soup.find('td', string='Gender').find_next('td').get_text(strip=True)\n",
    "            Gender.append(gender)\n",
    "        except:\n",
    "            Gender.append('')\n",
    "\n",
    "        try:\n",
    "            diameter = soup.find('td', string='Diameter (mm)').find_next('td').get_text(strip=True)\n",
    "            Diameter.append(diameter)\n",
    "        except:\n",
    "            Diameter.append('')\n",
    "\n",
    "        try:\n",
    "            original_box = soup.find('td', string='Original Box').find_next('td').get_text(strip=True)\n",
    "            originalBox.append(original_box)\n",
    "        except:\n",
    "            originalBox.append('')\n",
    "        \n",
    "        try:\n",
    "            original_papers = soup.find('td', string='Original Papers').find_next('td').get_text(strip=True)\n",
    "            originalPaper.append(original_papers)\n",
    "        except:\n",
    "            originalPaper.append('')\n",
    "        \n",
    "        c = c + 1\n",
    "        print(f\"Product {c} completed.\")\n",
    "\n",
    "        time.sleep(5)  # Sleep for 5 seconds between requests\n",
    "        \n",
    "        if c==15:\n",
    "            break\n",
    "        \n",
    "    # # Writing to CSV file\n",
    "    # with open('output.csv', 'w', encoding='utf-8', newline='') as file:\n",
    "    #     writer = csv.writer(file)\n",
    "    #     writer.writerow(['Image Urls', 'Product Name', 'Price', 'Model Number', 'Model Year', 'Gender', 'Diameter',\n",
    "    #                     'Original Box', 'Original Papers', 'Product Url'])\n",
    "\n",
    "    #     for i in range(len(productName)):\n",
    "    #         # Join image URLs into a single string separated by commas\n",
    "    #         img_urls_str = ', '.join(imgUrls[i])\n",
    "    #         writer.writerow([img_urls_str, productName[i], Price[i], modelNumber[i], modelYear[i], Gender[i],\n",
    "    #                         Diameter[i], originalBox[i], originalPaper[i], productUrls[i]])\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://watchcollectors.co.uk/products/choaprd', 'https://watchcollectors.co.uk/products/louis-vuitton-ref-tambour-xl-minute-repeater-travel-time-18k-rose-gold-box-papers', 'https://watchcollectors.co.uk/products/rolex-lady-datejust-pearlmaster-ref-69298-white-mother-of-pearl-diamond-dial-18k-yellow-gold', 'https://watchcollectors.co.uk/products/rolex-yacht-master-40-ref-126622-slate-dial-box-papers-2021-stainless-steel-platinum', 'https://watchcollectors.co.uk/products/audemars-piguet-royal-oak-chronograph-ref-26603st-oo-d002cr-01000-blue-dial-box-papers-2010-stainless-steel', 'https://watchcollectors.co.uk/products/rolex-daytona-ref-116520-black-dial-2012-box-papers-stainless-steel-1', 'https://watchcollectors.co.uk/products/rolex-daytona-beach-blue-ref-116509-18k-white-gold-2007', 'https://watchcollectors.co.uk/products/vacheron-constantin-malte-chronograph-ref-49180-champagne-dial-40mm-18k-rose-gold', 'https://watchcollectors.co.uk/products/cartier-tank-americaine-chronograph-ref-2892-automatic-31mm-18k-yellow-gold', 'https://watchcollectors.co.uk/products/cartier-tank-americaine-chronograph-ref-3072-automatic-31mm-18k-rose-gold', 'https://watchcollectors.co.uk/products/cartier-tank-americaine-ref-2927-automatic-31mm-18k-rose-gold', 'https://watchcollectors.co.uk/products/cartier-tank-americaine-chronograph-ref-3073-automatic-31mm-18k-white-gold']\n"
     ]
    }
   ],
   "source": [
    "url = input(\"Enter Url: \")\n",
    "all_links = products_links(url)\n",
    "\n",
    "# Print the list of links\n",
    "print(all_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "£165,000.00\n",
      "Product 1 completed.\n",
      "£165,000.00\n",
      "Product 2 completed.\n",
      "£165,000.00\n",
      "Product 3 completed.\n",
      "£165,000.00\n",
      "Product 4 completed.\n",
      "£165,000.00\n",
      "Product 5 completed.\n",
      "£165,000.00\n",
      "Product 6 completed.\n",
      "£165,000.00\n",
      "Product 7 completed.\n",
      "£165,000.00\n",
      "Product 8 completed.\n",
      "£165,000.00\n",
      "Product 9 completed.\n",
      "£165,000.00\n",
      "Product 10 completed.\n",
      "£165,000.00\n",
      "Product 11 completed.\n",
      "£165,000.00\n",
      "Product 12 completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Product_details(all_links)\n",
    "# Product_details(all_product_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Image Urls': [' '.join(img_urls) for img_urls in imgUrls],\n",
    "    'Product Name': productName,\n",
    "    'Price': Price,\n",
    "    'Model Number': modelNumber,\n",
    "    'Model Year': modelYear,\n",
    "    'Gender': Gender,\n",
    "    'Diameter': Diameter,\n",
    "    'Original Box': originalBox,\n",
    "    'Original Papers': originalPaper,\n",
    "    'Product Url': productUrls\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Writing to Excel file\n",
    "excel_file_path = 'output.xlsx'\n",
    "df.to_excel(excel_file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
